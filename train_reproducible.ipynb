{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import pathlib\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "import model_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "class trainArgs:\n",
    "    \"\"\"Argument class for training setup and parameters. Some advanced settings are hardcoded for simplicity.\"\"\"\n",
    "    \n",
    "    arch = None\n",
    "    world_size = -1\n",
    "    rank = -1\n",
    "    dist_url = 'tcp://224.66.41.62:23456'\n",
    "    dist_backend = 'ncc1'\n",
    "    multiprocessing_distributed = False\n",
    "    \n",
    "    def __init__(self, root, workers=4, epochs=90, start_epoch=0, batch_size=256, lr=0.1, lrdecay=0.1,\n",
    "                 momentum=0.9, weight_decay=1e-4, print_freq=10, resume='', main_file='candidate.npz',\n",
    "                 sub_file=None, test_file='candidate.npz', outpath='./results/unknown/train_logs.txt',\n",
    "                 model='resnet', inject_noise=0, mix_cifar=False, mix_rate=0.04, \n",
    "                 evaluate=False, track_correct=False, pretrained=False, seed=None, gpu=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (pathlib.Path): Directory containing the npz files.\n",
    "            main_file (npz filename): npz file that contains the training set in the 'X_train' field.\n",
    "            test_file (npz filename, optional): npz file that contains the training set in the 'X_test' field.\n",
    "                This is used in case the main file is for the CIFAR-10 dataset.\n",
    "            sub_file (npz filename, optional): npz file that contains the CIFAR-10 dataset.\n",
    "                If given, this is used for tracking the performance on a second test set.\n",
    "            outpath (pathlib.Path): Output log file for accuracy results.\n",
    "            inject_noise (optional, TODO): Symmetric noise level per class for injecting noise to CIFAR-10 training set.\n",
    "            mix_cifar (boolean, optional, TODO): Replace fraction of one training set with random images from another.\n",
    "            mix_rate (conditional, TODO): Determines the fraction when mix_cifar is set to True.\n",
    "            evaluate (boolean, optional): Evaluate the performance on the test set without training.\n",
    "            track_correct (boolean): Record the indices of correctly classified test images and write it to\n",
    "                a file with '_corr' added before the extension to the outpath filename.\n",
    "            gpu: Cuda device for training.\n",
    "            \n",
    "            train (boolean): Extract train (True) or test (False) set from the file.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.workers = workers\n",
    "        self.epochs = int(epochs)\n",
    "        self.start_epoch = start_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.lrdecay = lrdecay\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.print_freq = print_freq\n",
    "        self.resume = resume\n",
    "        self.main_file = main_file\n",
    "        self.sub_file = sub_file\n",
    "        self.test_file = test_file\n",
    "        self.outpath = outpath\n",
    "        self.model = model\n",
    "        self.inject_noise = inject_noise\n",
    "        self.mix_cifar = mix_cifar\n",
    "        self.mix_rate = mix_rate\n",
    "        self.evaluate = evaluate\n",
    "        self.track_correct = track_correct\n",
    "        self.pretrained = pretrained\n",
    "        self.seed = seed\n",
    "        self.gpu = gpu\n",
    "\n",
    "best_acc1 = 0\n",
    "\n",
    "class CandidateDataset(Dataset):\n",
    "    \"\"\"Candidate dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, pathname, transform=None, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pathname (pathlib.Path): Path to the npz file.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            train (boolean): Extract train (True) or test (False) set from the file.\n",
    "        \"\"\"\n",
    "        self.samples, self.targets = np_loader(pathname.resolve(), train=train)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (sample, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        sample, target = self.samples[index], self.targets[index]\n",
    "        sample = Image.fromarray(np.moveaxis(sample, 0, -1))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        # TODO: Target transform.\n",
    "        \n",
    "        return sample, target\n",
    "    \n",
    "def np_loader(filename, train=True):\n",
    "    data = np.load(filename)\n",
    "    if train:\n",
    "        samples = data['X_train'].transpose(0, 3, 1, 2)\n",
    "        targets = data['y_train']\n",
    "    else:\n",
    "        samples = data['X_test'].transpose(0, 3, 1, 2)\n",
    "        targets = data['y_test']\n",
    "    return samples, targets\n",
    "\n",
    "def main_worker(gpu, ngpus_per_node, args):\n",
    "    global best_acc1\n",
    "\n",
    "    if args.gpu is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
    "\n",
    "    if args.distributed:\n",
    "        if args.dist_url == \"env://\" and args.rank == -1:\n",
    "            args.rank = int(os.environ[\"RANK\"])\n",
    "        if args.multiprocessing_distributed:\n",
    "            # For multiprocessing distributed training, rank needs to be the\n",
    "            # global rank among all the processes\n",
    "            args.rank = args.rank * ngpus_per_node + gpu\n",
    "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                world_size=args.world_size, rank=args.rank)\n",
    "    # create model\n",
    "    if args.pretrained:\n",
    "        print(\"=> using pre-trained model '{}'\".format(args.arch))\n",
    "        model = models.__dict__[args.arch](pretrained=True)\n",
    "    else:\n",
    "        print(\"=> creating model '{}'\".format(args.model))\n",
    "        model = model_select.BaseModel.create(args.model)\n",
    "    \n",
    "    if args.distributed:\n",
    "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "        # should always set the single device scope, otherwise,\n",
    "        # DistributedDataParallel will use all available devices.\n",
    "        if args.gpu is not None:\n",
    "            torch.cuda.set_device(args.gpu)\n",
    "            model.cuda(args.gpu)\n",
    "            # When using a single GPU per process and per\n",
    "            # DistributedDataParallel, we need to divide the batch size\n",
    "            # ourselves based on the total number of GPUs we have\n",
    "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
    "            args.workers = int(args.workers / ngpus_per_node)\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "        else:\n",
    "            model.cuda()\n",
    "            # DistributedDataParallel will divide and allocate batch_size to all\n",
    "            # available GPUs if device_ids are not set\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    elif args.gpu is not None:\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        model = model.cuda(args.gpu)\n",
    "    else:\n",
    "        # DataParallel will divide and allocate batch_size to all available GPUs\n",
    "        if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):\n",
    "            model.features = torch.nn.DataParallel(model.features)\n",
    "            model.cuda()\n",
    "        else:\n",
    "            model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda(args.gpu)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay)\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_acc1 = checkpoint['best_acc1']\n",
    "            if args.gpu is not None:\n",
    "                # best_acc1 may be from a checkpoint from a different GPU\n",
    "                best_acc1 = best_acc1.to(args.gpu)\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Data loading code\n",
    "    main_file = args.root / args.main_file\n",
    "    if args.sub_file:\n",
    "        sub_file = args.root / args.sub_file\n",
    "    normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                                     std=[0.2023, 0.1994, 0.2010])\n",
    "\n",
    "    train_dataset = CandidateDataset(\n",
    "        main_file,\n",
    "        transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "        \n",
    "    # TODO: Inject symmetric noise to CIFAR-10 training set\n",
    "    if args.inject_noise:\n",
    "        im_per_class = int(len(train_dataset) / len(cifar10_classes))\n",
    "        noisy_idx = []\n",
    "        num_shuffle = int(im_per_class * args.inject_noise)\n",
    "        for i in range(len(cifar10_classes)):\n",
    "            cur_idx = [idx for idx, label in enumerate(train_dataset.targets) if label==i]\n",
    "            cur_idx = random.sample(cur_idx, len(cur_idx))\n",
    "            for r in range(len(cifar10_classes)):\n",
    "                noisy_idx += [r for idx in cur_idx[im_per_class - (r+1)*num_shuffle:im_per_class - r*num_shuffle]]\n",
    "            noisy_idx += [i for idx in cur_idx[:im_per_class - len(cifar10_classes)*num_shuffle]]\n",
    "        train_dataset.targets = noisy_idx\n",
    "    \n",
    "    # TODO: Replace fraction of one training set randomly with another.\n",
    "    if args.mix_cifar:\n",
    "        assert args.mix_rate, \"mix_rate should be given when mix_cifar is set\"\n",
    "        assert args.traindir2, \"traindir2 must be given when mix_cifar is set\"\n",
    "        assert not args.inject_noise, \"inject_noise should not be given when mix_cifar is set\"\n",
    "        assert not args.testdir2, \"only one testdir can be set when mix_cifar is set\"\n",
    "        \n",
    "        traindir2 = os.path.join(args.root, args.traindir2)\n",
    "        clean_dataset = datasets.ImageFolder(\n",
    "            traindir2,\n",
    "            transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]))\n",
    "        \n",
    "        im_per_class = int(len(train_dataset) / len(train_dataset.classes))\n",
    "        num_shuffle = int(im_per_class * args.mix_rate)\n",
    "        shuffled_samples = []\n",
    "        clean_samples = []\n",
    "        for i in range(len(train_dataset.classes)):\n",
    "            cur_imgs = [s[0] for s in train_dataset.samples if s[1]==i]\n",
    "            cur_imgs = random.sample(cur_imgs, im_per_class - num_shuffle)\n",
    "            mix_imgs = [s[0] for s in clean_dataset.samples if s[1]==i]\n",
    "            mix_imgs = random.sample(mix_imgs, num_shuffle)\n",
    "            clean_samples += [(img, i) for img in mix_imgs]\n",
    "            shuffled_samples += [(img, i) for img in cur_imgs + mix_imgs]\n",
    "            \n",
    "        train_dataset.samples = shuffled_samples\n",
    "        clean_dataset.samples = clean_samples\n",
    "        \n",
    "        val_loader2 = torch.utils.data.DataLoader(\n",
    "            clean_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),\n",
    "            num_workers=args.workers, pin_memory=True, sampler=train_sampler)\n",
    "        \n",
    "    if args.distributed:\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "    else:\n",
    "        train_sampler = None\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),\n",
    "        num_workers=args.workers, pin_memory=True, sampler=train_sampler)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        CandidateDataset(main_file, transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]), train=False),\n",
    "        batch_size=args.batch_size, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True)\n",
    "    \n",
    "    if args.sub_file:\n",
    "        val_loader2 = torch.utils.data.DataLoader(\n",
    "            CandidateDataset(sub_file, transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]), train=False),\n",
    "            batch_size=args.batch_size, shuffle=False,\n",
    "            num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    if args.evaluate:\n",
    "        validate(val_loader, model, criterion, args)\n",
    "        return\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        if args.distributed:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "        if epoch < 70:\n",
    "            adjust_learning_rate(optimizer, epoch, args)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch, args)\n",
    "        \n",
    "        with open(args.outpath, 'a') as fn:\n",
    "            print('Epoch {}'.format(epoch), file = fn)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        dum_acc = validate(train_loader, model, criterion, args)\n",
    "        acc1 = validate(val_loader, model, criterion, args)\n",
    "        if args.sub_file or args.mix_cifar:\n",
    "            dum_acc = validate(val_loader2, model, criterion, args)\n",
    "\n",
    "        # remember best acc@1 and save checkpoint\n",
    "        is_best = acc1 > best_acc1\n",
    "        best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "        if not args.multiprocessing_distributed or (args.multiprocessing_distributed\n",
    "                and args.rank % ngpus_per_node == 0):\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'arch': args.arch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_acc1': best_acc1,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best)\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, args):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if args.gpu is not None:\n",
    "            input = input.cuda(args.gpu, non_blocking=True)\n",
    "        target = target.cuda(args.gpu, non_blocking=True)\n",
    "        \n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(acc1[0], input.size(0))\n",
    "        top5.update(acc5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, args):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    if args.track_correct:\n",
    "        corr_dict = {'correct':[]}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            if args.gpu is not None:\n",
    "                input = input.cuda(args.gpu, non_blocking=True)\n",
    "            target = target.cuda(args.gpu, non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # record correctly classified examples\n",
    "            if args.track_correct:\n",
    "                correct = accuracy(output, target, topk=(1, 5), track=True)\n",
    "                corr_dict['correct'] += [(i*args.batch_size) + idx for idx, is_corr in \n",
    "                                         enumerate(correct) if is_corr]\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(acc1[0], input.size(0))\n",
    "            top5.update(acc5[0], input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                      'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                       i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                       top1=top1, top5=top5))\n",
    "\n",
    "        # Record the indices of the correctly classified images\n",
    "        if args.track_correct:\n",
    "            fname, ext = str(args.outpath).split('.')\n",
    "            corrfile = fname + '_corr.json'\n",
    "            with open(corrfile, 'w') as f:\n",
    "                json.dump(corr_dict, f, indent=2)\n",
    "            return\n",
    "        \n",
    "        with open(args.outpath, 'a') as fn:\n",
    "            print('{top1.avg:.3f}, {top5.avg:.3f}'\n",
    "                  .format(top1=top1, top5=top5), file = fn)\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * (args.lrdecay ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,), track=False):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        \n",
    "        # return indices of the correctly classified examples instead of accuracy.\n",
    "        if track:\n",
    "            return correct[:1].view(-1).cpu().numpy()\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "# directories\n",
    "root = pathlib.Path.cwd()\n",
    "main_file = 'candidate.npz'\n",
    "sub_file = 'cifar.npz'\n",
    "\n",
    "# TODO: Experiments with synthetic noise and fractions\n",
    "# mix_cifar = True\n",
    "# inject_noise = 0.05\n",
    "\n",
    "# model and output\n",
    "model = 'resnet'\n",
    "outpath = pathlib.Path.cwd() / 'results' / model / 'train_logs.txt'\n",
    "\n",
    "# evaluate mode with recording indices\n",
    "# evaluate = True\n",
    "# track_correct=True\n",
    "# resume_file = 'model_best.pth.tar'\n",
    "\n",
    "# run parameters\n",
    "epochs = 50\n",
    "print_freq = 1000\n",
    "gpu = 0\n",
    "batch_size = 128 if model in ['densenet', 'pyramidnet', 'resnet_basic'] else 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = trainArgs(root, epochs=epochs, batch_size=batch_size, print_freq=print_freq, outpath=outpath,\n",
    "                 main_file=main_file,\n",
    "                 sub_file=sub_file, \n",
    "                 # evaluate=evaluate, \n",
    "                 # track_correct=track_correct, \n",
    "                 # resume=resume_file, \n",
    "                 model=model, gpu=gpu)\n",
    "\n",
    "args.outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "if args.seed is not None:\n",
    "    random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    cudnn.deterministic = True\n",
    "    warnings.warn('You have chosen to seed training. '\n",
    "                  'This will turn on the CUDNN deterministic setting, '\n",
    "                  'which can slow down your training considerably! '\n",
    "                  'You may see unexpected behavior when restarting '\n",
    "                  'from checkpoints.')\n",
    "\n",
    "if args.gpu is not None:\n",
    "    warnings.warn('You have chosen a specific GPU. This will completely '\n",
    "                  'disable data parallelism.')\n",
    "\n",
    "if args.dist_url == \"env://\" and args.world_size == -1:\n",
    "    args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
    "\n",
    "ngpus_per_node = torch.cuda.device_count()\n",
    "if args.multiprocessing_distributed:\n",
    "    # Since we have ngpus_per_node processes per node, the total world_size\n",
    "    # needs to be adjusted accordingly\n",
    "    args.world_size = ngpus_per_node * args.world_size\n",
    "    # Use torch.multiprocessing.spawn to launch distributed processes: the\n",
    "    # main_worker process function\n",
    "    mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n",
    "else:\n",
    "    # Simply call main_worker function\n",
    "    main_worker(args.gpu, ngpus_per_node, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def get_item_test():\n",
    "    normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                                         std=[0.2023, 0.1994, 0.2010])\n",
    "    set_trace()\n",
    "    \n",
    "    train_dataset = CandidateDataset(\n",
    "            root / main_file,\n",
    "            transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]))\n",
    "    \n",
    "    train_dataset2 = datasets.ImageFolder(\n",
    "        '/root/dockspace/cifar/tiny_train/',\n",
    "        transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    \n",
    "    test_sample = train_dataset[0]\n",
    "    test_sample2 = train_dataset2[0]\n",
    "    i = 1\n",
    "    return test_sample\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_item_test()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
